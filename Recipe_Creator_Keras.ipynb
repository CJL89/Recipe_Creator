{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing dependecies:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "from pandas import compat\n",
    "compat.PY3 = True\n",
    "\n",
    "# Configuring Notebook environment:\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.5)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>ingredients_vector</th>\n",
       "      <th>instructions_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p3pKOD6jIHEcjf20CCXohP8uqkG5dGi</th>\n",
       "      <td>grammie hamblets deviled crab</td>\n",
       "      <td>celery finely chopped green pepper finely chop...</td>\n",
       "      <td>toss ingredients lightly spoon buttered baking...</td>\n",
       "      <td>['celery', 'finely', 'chopped', 'green', 'pepp...</td>\n",
       "      <td>['toss', 'ingredients', 'lightly', 'spoon', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO</th>\n",
       "      <td>infineon raceway baked beans</td>\n",
       "      <td>skirt steak cut inch dicekosher salt fresh cra...</td>\n",
       "      <td>watch make recipe sprinkle steak salt pepper s...</td>\n",
       "      <td>['skirt', 'steak', 'cut', 'inch', 'dicekosher'...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'sprinkle', 'steak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK</th>\n",
       "      <td>southwestern black bean dip</td>\n",
       "      <td>cups dried black beans picked rinsed cups wate...</td>\n",
       "      <td>saucepan let beans soak enough cold water cove...</td>\n",
       "      <td>['cups', 'dried', 'black', 'beans', 'picked', ...</td>\n",
       "      <td>['saucepan', 'let', 'beans', 'soak', 'enough',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5l1yTSYFifF/M2dfbD6DX28WWQpLWNK</th>\n",
       "      <td>sour cream noodle bake</td>\n",
       "      <td>ground chuckone tomato sauce saltfreshly groun...</td>\n",
       "      <td>watch make recipe preheat oven degrees f brown...</td>\n",
       "      <td>['ground', 'chuckone', 'tomato', 'sauce', 'sal...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'preheat', 'oven',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kRBQSWtqYWqtkb34FGeenBSbC32gIdO</th>\n",
       "      <td>sushi renovation</td>\n",
       "      <td>rice brown mediumgrain cookedcup quinoacup swe...</td>\n",
       "      <td>special equipment sushi mat cook brown rice qu...</td>\n",
       "      <td>['rice', 'brown', 'mediumgrain', 'cookedcup', ...</td>\n",
       "      <td>['special', 'equipment', 'sushi', 'mat', 'cook...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         title  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  grammie hamblets deviled crab   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   infineon raceway baked beans   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    southwestern black bean dip   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         sour cream noodle bake   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               sushi renovation   \n",
       "\n",
       "                                                                       ingredients  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  celery finely chopped green pepper finely chop...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  skirt steak cut inch dicekosher salt fresh cra...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  cups dried black beans picked rinsed cups wate...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ground chuckone tomato sauce saltfreshly groun...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  rice brown mediumgrain cookedcup quinoacup swe...   \n",
       "\n",
       "                                                                      instructions  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  toss ingredients lightly spoon buttered baking...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  watch make recipe sprinkle steak salt pepper s...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  saucepan let beans soak enough cold water cove...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  watch make recipe preheat oven degrees f brown...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  special equipment sushi mat cook brown rice qu...   \n",
       "\n",
       "                                                                ingredients_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['celery', 'finely', 'chopped', 'green', 'pepp...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['cups', 'dried', 'black', 'beans', 'picked', ...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['ground', 'chuckone', 'tomato', 'sauce', 'sal...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['rice', 'brown', 'mediumgrain', 'cookedcup', ...   \n",
       "\n",
       "                                                               instructions_vector  \n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['toss', 'ingredients', 'lightly', 'spoon', 'b...  \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['watch', 'make', 'recipe', 'sprinkle', 'steak...  \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['saucepan', 'let', 'beans', 'soak', 'enough',...  \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['watch', 'make', 'recipe', 'preheat', 'oven',...  \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['special', 'equipment', 'sushi', 'mat', 'cook...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/strings/df_clean.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>ingredients_vector</th>\n",
       "      <th>instructions_vector</th>\n",
       "      <th>title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p3pKOD6jIHEcjf20CCXohP8uqkG5dGi</th>\n",
       "      <td>grammie hamblets deviled crab</td>\n",
       "      <td>celery finely chopped green pepper finely chop...</td>\n",
       "      <td>toss ingredients lightly spoon buttered baking...</td>\n",
       "      <td>['celery', 'finely', 'chopped', 'green', 'pepp...</td>\n",
       "      <td>['toss', 'ingredients', 'lightly', 'spoon', 'b...</td>\n",
       "      <td>['grammie hamblets deviled crab']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO</th>\n",
       "      <td>infineon raceway baked beans</td>\n",
       "      <td>skirt steak cut inch dicekosher salt fresh cra...</td>\n",
       "      <td>watch make recipe sprinkle steak salt pepper s...</td>\n",
       "      <td>['skirt', 'steak', 'cut', 'inch', 'dicekosher'...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'sprinkle', 'steak...</td>\n",
       "      <td>['infineon raceway baked beans']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK</th>\n",
       "      <td>southwestern black bean dip</td>\n",
       "      <td>cups dried black beans picked rinsed cups wate...</td>\n",
       "      <td>saucepan let beans soak enough cold water cove...</td>\n",
       "      <td>['cups', 'dried', 'black', 'beans', 'picked', ...</td>\n",
       "      <td>['saucepan', 'let', 'beans', 'soak', 'enough',...</td>\n",
       "      <td>['southwestern black bean dip']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5l1yTSYFifF/M2dfbD6DX28WWQpLWNK</th>\n",
       "      <td>sour cream noodle bake</td>\n",
       "      <td>ground chuckone tomato sauce saltfreshly groun...</td>\n",
       "      <td>watch make recipe preheat oven degrees f brown...</td>\n",
       "      <td>['ground', 'chuckone', 'tomato', 'sauce', 'sal...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'preheat', 'oven',...</td>\n",
       "      <td>['sour cream noodle bake']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kRBQSWtqYWqtkb34FGeenBSbC32gIdO</th>\n",
       "      <td>sushi renovation</td>\n",
       "      <td>rice brown mediumgrain cookedcup quinoacup swe...</td>\n",
       "      <td>special equipment sushi mat cook brown rice qu...</td>\n",
       "      <td>['rice', 'brown', 'mediumgrain', 'cookedcup', ...</td>\n",
       "      <td>['special', 'equipment', 'sushi', 'mat', 'cook...</td>\n",
       "      <td>['sushi renovation']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         title  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  grammie hamblets deviled crab   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   infineon raceway baked beans   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    southwestern black bean dip   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         sour cream noodle bake   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               sushi renovation   \n",
       "\n",
       "                                                                       ingredients  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  celery finely chopped green pepper finely chop...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  skirt steak cut inch dicekosher salt fresh cra...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  cups dried black beans picked rinsed cups wate...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ground chuckone tomato sauce saltfreshly groun...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  rice brown mediumgrain cookedcup quinoacup swe...   \n",
       "\n",
       "                                                                      instructions  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  toss ingredients lightly spoon buttered baking...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  watch make recipe sprinkle steak salt pepper s...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  saucepan let beans soak enough cold water cove...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  watch make recipe preheat oven degrees f brown...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  special equipment sushi mat cook brown rice qu...   \n",
       "\n",
       "                                                                ingredients_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['celery', 'finely', 'chopped', 'green', 'pepp...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['cups', 'dried', 'black', 'beans', 'picked', ...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['ground', 'chuckone', 'tomato', 'sauce', 'sal...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['rice', 'brown', 'mediumgrain', 'cookedcup', ...   \n",
       "\n",
       "                                                               instructions_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['toss', 'ingredients', 'lightly', 'spoon', 'b...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['watch', 'make', 'recipe', 'sprinkle', 'steak...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['saucepan', 'let', 'beans', 'soak', 'enough',...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['watch', 'make', 'recipe', 'preheat', 'oven',...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['special', 'equipment', 'sushi', 'mat', 'cook...   \n",
       "\n",
       "                                                   title_tokenized  \n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['grammie hamblets deviled crab']  \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   ['infineon raceway baked beans']  \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    ['southwestern black bean dip']  \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         ['sour cream noodle bake']  \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               ['sushi renovation']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_tokenized'] = list(df['title'].apply(sent_tokenize).astype(str))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Splitting Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi    ['celery', 'finely', 'chopped', 'green', 'pepp...\n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO    ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...\n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    ['cups', 'dried', 'black', 'beans', 'picked', ...\n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK    ['ground', 'chuckone', 'tomato', 'sauce', 'sal...\n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO    ['rice', 'brown', 'mediumgrain', 'cookedcup', ...\n",
       "                                                         ...                        \n",
       "4bfMWxlbKhx/McJq/89k0SBdw.VvAzW    ['ears', 'fresh', 'corn', 'heads', 'belgian', ...\n",
       "T8lWBA1fcVdjxhMSWuoAbGoy5Lj.A8m    ['plum', 'tomatoessalt', 'sugar', 'zucchini', ...\n",
       "f/coffo2TMs2J2gq5nTOUIqH2TRAkui    ['tablespoons', 'olive', 'oil', 'tablespoons',...\n",
       "q3aDJc4zoEF5QT4e7Mn.ieQwV.DyHwS    ['ounces', 'butter', 'ounces', 'bittersweet', ...\n",
       "7cXA77UpdDtIfBug2v6lEVIuV3Zcvhm    ['cans', 'restaurantstyle', 'condensed', 'crab...\n",
       "Name: ingredients_vector, Length: 59612, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['ingredients_vector']\n",
    "y = df['title_tokenized']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47689 train sequences\n",
      "11923 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47690"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(y_train) + 1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Split Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer.fit_on_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = []\n",
    "for text in X_train:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train.factorize()[0], num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test.factorize()[0], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (47689, 47690)\n",
      "y_test shape: (11923, 47690)\n"
     ]
    }
   ],
   "source": [
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (47689, 1000)\n",
      "X_test shape: (11923, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 100\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 42920 samples, validate on 4769 samples\n",
      "Epoch 1/8\n",
      "42920/42920 [==============================] - 244s 6ms/step - loss: 10.8117 - acc: 0.0024 - val_loss: 10.8257 - val_acc: 0.0059\n",
      "Epoch 2/8\n",
      "42920/42920 [==============================] - 252s 6ms/step - loss: 8.4440 - acc: 0.0515 - val_loss: 10.8512 - val_acc: 0.0398\n",
      "Epoch 3/8\n",
      "42920/42920 [==============================] - 260s 6ms/step - loss: 4.2025 - acc: 0.3377 - val_loss: 11.0066 - val_acc: 0.0673\n",
      "Epoch 4/8\n",
      "42920/42920 [==============================] - 254s 6ms/step - loss: 1.7319 - acc: 0.6656 - val_loss: 11.5066 - val_acc: 0.0734\n",
      "Epoch 5/8\n",
      "42920/42920 [==============================] - 242s 6ms/step - loss: 0.8470 - acc: 0.8319 - val_loss: 12.1505 - val_acc: 0.0744\n",
      "Epoch 6/8\n",
      "42920/42920 [==============================] - 252s 6ms/step - loss: 0.5526 - acc: 0.8952 - val_loss: 12.7071 - val_acc: 0.0736\n",
      "Epoch 7/8\n",
      "42920/42920 [==============================] - 238s 6ms/step - loss: 0.4384 - acc: 0.9225 - val_loss: 13.0961 - val_acc: 0.0753\n",
      "Epoch 8/8\n",
      "42920/42920 [==============================] - 231s 5ms/step - loss: 0.3790 - acc: 0.9351 - val_loss: 13.4533 - val_acc: 0.0746\n",
      " 4700/11923 [==========>...................] - ETA: 52s"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(256, input_shape=(max_words,), activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(2, input_shape=(num_classes), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
