{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### 1.0 Importing Dependecies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "#configure\n",
    "import nltk\n",
    "\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# tokenizing\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, Activation, LSTM\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.5)\n",
    "pd.set_option('display.max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 59,
        "hidden": false,
        "row": 0,
        "width": 8
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 59628 entries, p3pKOD6jIHEcjf20CCXohP8uqkG5dGi to 7cXA77UpdDtIfBug2v6lEVIuV3Zcvhm\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   title                59628 non-null  object\n",
      " 1   ingredients          58153 non-null  object\n",
      " 2   instructions         59612 non-null  object\n",
      " 3   ingredients_vector   59628 non-null  object\n",
      " 4   instructions_vector  59628 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/strings/df_clean.csv', index_col=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 59628 entries, p3pKOD6jIHEcjf20CCXohP8uqkG5dGi to 7cXA77UpdDtIfBug2v6lEVIuV3Zcvhm\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   title                59628 non-null  object\n",
      " 1   ingredients          59628 non-null  object\n",
      " 2   instructions         59612 non-null  object\n",
      " 3   ingredients_vector   59628 non-null  object\n",
      " 4   instructions_vector  59628 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df[['title', 'ingredients']] = df[['title', 'ingredients']].astype(str)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### 2.0 Defining and Splitting Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X = tfidf_vec.fit_transform(df['ingredients'])\n",
    "y = label_encoder.fit_transform(df['title'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (39950, 44673)\n",
      "Test:  (19678, 44673)\n",
      "DF:  (59628, 44673)\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', X_train.shape)\n",
    "print('Test: ', X_test.shape)\n",
    "print('DF: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 8,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### Machine Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "bayes = MultinomialNB()\n",
    "bayes.fit(X_train, y_train)\n",
    "predictions = bayes.predict(X_test)\n",
    "precision_score(predictions, y_test, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {'sgd': SGDClassifier(loss='hinge'),\n",
    "               'svm': SVC(),\n",
    "               'random_forest': RandomForestClassifier()}\n",
    "\n",
    "for lbl, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(lbl, precision_score(predictions, y_test, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import eye\n",
    "d = eye(len(tfidf_vec.vocabulary_))\n",
    "word_pred = bayes.predict_proba(d)\n",
    "inverse_vocab = {idx: word for word, idx in tfidf_vec.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "by_cls = defaultdict(Counter)\n",
    "for word_idx, pred in enumerate(word_pred):\n",
    "    for class_idx, score in enumerate(pred):\n",
    "        cls = label_encoder.classes_[class_idx]\n",
    "        by_cls[cls][inverse_vocab[word_idx]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "for k in by_cls:\n",
    "    words = [x[0] for x in by_cls[k].most_common(5)]\n",
    "    print(k, ':', ' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 12,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### Deep Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepping Non Vectorized Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Taking all the different characters found in the DF:\n",
    "chars = list(sorted(set(chain(*df['ingredients']))))\n",
    "# Giving a unique index to all characters found:\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "# Taking the longest sequence:\n",
    "max_sequence_len = max(len(x) for x in df['ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "char_vectors = []\n",
    "\n",
    "# Creating vectors for each title:\n",
    "for txt in df['ingredients']:\n",
    "    vec = np.zeros((max_sequence_len, len(char_to_idx)))\n",
    "    vec[np.arange(len(txt)), [char_to_idx[ch] for ch in txt]] = 1\n",
    "    char_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming vectors to arrays:\n",
    "char_vectors = [np.asarray(x) for x in char_vectors]\n",
    "char_vectors = pad_sequences(char_vectors)\n",
    "\n",
    "# Encoding titles:\n",
    "labels = label_encoder.transform(df['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59628, 2311, 48)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting data into X & Y train test:\n",
    "def split(lst):\n",
    "    training_count = int(0.9 * len(char_vectors))\n",
    "    return lst[:training_count], lst[training_count:]\n",
    "\n",
    "training_char_vectors, test_char_vectors = split(char_vectors)\n",
    "training_labels, test_labels = split(labels)\n",
    "\n",
    "char_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 21,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### Char CNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 2311, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2306, 128)         36992     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 384, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 379, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 63, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16128)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               2064512   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 52474)             6769146   \n",
      "=================================================================\n",
      "Total params: 9,067,514\n",
      "Trainable params: 9,067,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Concatenate\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    conv_1x = Conv1D(128, 6, activation='relu', padding='valid')(char_input)\n",
    "    max_pool_1x = MaxPooling1D(6)(conv_1x)\n",
    "    conv_2x = Conv1D(256, 6, activation='relu', padding='valid')(max_pool_1x)\n",
    "    max_pool_2x = MaxPooling1D(6)(conv_2x)\n",
    "\n",
    "    flatten = Flatten()(max_pool_2x)\n",
    "    dense = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.01))(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "char_cnn_model.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "char_cnn_model.save(\"char_cnn_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5963/5963 [==============================] - 53s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.64173422158814, 0.0001677008217340265]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "char_cnn_model_loaded = load_model('char_cnn_model.h5')\n",
    "# evaluating model\n",
    "char_cnn_model_loaded.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### CNN CNN Model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 2311, 48)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2307, 128)    30848       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2306, 128)    36992       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2305, 128)    43136       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 461, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 384, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 329, 128)     0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 461, 128)     0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 384, 128)     0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 329, 128)     0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 457, 128)     82048       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 379, 128)     98432       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 323, 128)     114816      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 91, 128)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 63, 128)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 46, 128)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 91, 128)      0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 63, 128)      0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 46, 128)      0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200, 128)     0           dropout_2[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25600)        0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          3276928     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 52474)        6769146     dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,452,346\n",
      "Trainable params: 10,452,346\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "def create_char_cnn_model(num_chars, max_sequence_len, num_labels):\n",
    "    char_input = Input(shape=(max_sequence_len, num_chars), name='input')\n",
    "    \n",
    "    layers = []\n",
    "    for window in (5, 6, 7):\n",
    "        conv_1x = Conv1D(128, window, activation='relu', padding='valid')(char_input)\n",
    "        max_pool_1x = MaxPooling1D(window)(conv_1x)\n",
    "        dropout_1x = Dropout(0.3)(max_pool_1x)\n",
    "        conv_2x = Conv1D(128, window, activation='relu', padding='valid')(dropout_1x)\n",
    "        max_pool_2x = MaxPooling1D(window)(conv_2x)\n",
    "        dropout_2x = Dropout(0.3)(max_pool_2x)\n",
    "        layers.append(dropout_2x)\n",
    "\n",
    "    if len(layers) > 1:\n",
    "        merged = Concatenate(axis=1)(layers)\n",
    "    else:\n",
    "        merged = layers[0]\n",
    "\n",
    "    dropout = Dropout(0.3)(merged)\n",
    "    \n",
    "    flatten = Flatten()(dropout)\n",
    "    dense = Dense(128, activation='relu')(flatten)\n",
    "    preds = Dense(num_labels, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(char_input, preds)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "char_cnn_model_2 = create_char_cnn_model(len(char_to_idx), char_vectors.shape[1], len(label_encoder.classes_))\n",
    "char_cnn_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "char_cnn_model_2.fit(training_char_vectors, training_labels, epochs=20, batch_size=1024)\n",
    "char_cnn_model_2.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "char_cnn_model_2.save(\"char_cnn_model_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5963/5963 [==============================] - 54s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[14.684636452844554, 0.0011739057521381855]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "char_cnn_model_2_loaded = load_model('char_cnn_model_2.h5')\n",
    "# evaluate model\n",
    "char_cnn_model_2_loaded.evaluate(test_char_vectors, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on Tokenized Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df['ingredients_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import re\n",
    "\n",
    "CACHE_DIR = os.path.expanduser('~/.cache/dl-cookbook')\n",
    "\n",
    "def download(url):\n",
    "    filename = os.path.join(CACHE_DIR, re.sub('[^a-zA-Z0-9.]+', '_', url))\n",
    "    if os.path.exists(filename):\n",
    "        return filename\n",
    "    else:\n",
    "        os.system('mkdir -p \"%s\"' % CACHE_DIR)\n",
    "        assert os.system('wget -O \"%s\" \"%s\"' % (filename, url)) == 0\n",
    "        return filename\n",
    "\n",
    "def load_w2v(tokenizer=None):\n",
    "    word2vec_gz = download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "    word2vec_vectors = word2vec_gz.replace('.gz', '')\n",
    "    if not os.path.exists(word2vec_vectors):\n",
    "        assert os.system('gunzip -d --keep \"%s\"' % word2vec_gz) == 0\n",
    "        \n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_vectors, binary=True)\n",
    "    \n",
    "    total_count = sum(tokenizer.word_counts.values())\n",
    "    idf_dict = { k: np.log(total_count/v) for (k,v) in tokenizer.word_counts.items() }\n",
    "    \n",
    "    w2v = np.zeros((tokenizer.num_words, w2v_model.syn0.shape[1]))\n",
    "    idf = np.zeros((tokenizer.num_words, 1))\n",
    "\n",
    "    for k, v in tokenizer.word_index.items():\n",
    "        if v >= tokenizer.num_words:\n",
    "            continue\n",
    "\n",
    "        if k in w2v_model:\n",
    "            w2v[v] = w2v_model[k]\n",
    "            idf[v] = idf_dict[k]\n",
    "\n",
    "    del w2v_model\n",
    "    return w2v, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v, idf = load_w2v(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(df['ingredients_vector'])\n",
    "tokens = pad_sequences(tokens)\n",
    "\n",
    "training_count = int(0.9 * len(tokens))\n",
    "\n",
    "training_tokens, training_labels = tokens[:training_count], labels[:training_count]\n",
    "test_tokens, test_labels = tokens[training_count:], labels[training_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training With Vectorized Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "message (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 300)    15000000    message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 300)    0           message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "message_idf/embedding (Embeddin (None, None, 1)      50000       message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 300)          0           masking_1[0][0]                  \n",
      "                                                                 message_idf/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          38528       combine_and_sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 52474)        6769146     dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,857,674\n",
      "Trainable params: 6,807,674\n",
      "Non-trainable params: 15,050,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
    "    if weights is not None:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=weights.shape[1], \n",
    "                                weights=[weights], trainable=False, \n",
    "                                name='%s/embedding' % name)\n",
    "    else:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=embedding_size,\n",
    "                                name='%s/embedding' % name)\n",
    "\n",
    "def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):\n",
    "    assert not (embedding_size is None and embedding_weights is None)\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='message')\n",
    "    \n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)\n",
    "    idf = make_embedding('message_idf', vocab_size, embedding_size, idf_weights)\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        embedding, idf = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "    sum_msg = sum_layer([mask(embedding(message)), idf(message)])\n",
    "    fc1 = layers.Dense(units=128, activation='relu')(sum_msg)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=categories,\n",
    "    )\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,\n",
    "                                     embedding_weights=w2v,\n",
    "                                     idf_weights=idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model.fit(training_tokens, training_labels, epochs=10)\n",
    "unigram_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "unigram_model.save(\"unigram_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.765235011497241, 0.000670803286936106]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "unigram_model_loaded = load_model('unigram_model.h5')\n",
    "# evaluate model\n",
    "unigram_model_loaded.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "message (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None, 25)     0           message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "message_idf/embedding (Embeddin (None, None, 25)     1250000     message[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)        (None, 25)           0           masking_2[0][0]                  \n",
      "                                                                 message_idf/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          3328        combine_and_sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 52474)        6769146     dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,272,474\n",
      "Trainable params: 9,272,474\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)\n",
    "learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "learned_embeddings_model.save(\"learned_embeddings_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.91387734916133, 0.008049639443233272]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "learned_embeddings_model_loaded = load_model('learned_embeddings_model.h5')\n",
    "# evaluate model\n",
    "learned_embeddings_model_loaded.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights,\n",
    "                              mask_zero=False)\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(128, 3)\n",
    "    cnn_2 = layers.Convolution1D(128, 3)\n",
    "    cnn_3 = layers.Convolution1D(128, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)\n",
    "\n",
    "    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))\n",
    "    fc1 = layers.Dense(units=128, activation='elu')(cnn_encoding)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[categories],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "title (InputLayer)              (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "message_vec/embedding (Embeddin (None, None, 300)    15000000    title[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 128)    115328      message_vec/embedding[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, None, 128)    0           conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    49280       max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 128)    49280       max_pooling1d_11[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          16512       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 52474)        6769146     dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,999,546\n",
      "Trainable params: 6,999,546\n",
      "Non-trainable params: 15,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=10)\n",
    "cnn_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "cnn_model.save(\"cnn_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.793557491075386, 0.000670803286936106]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "cnn_model_loaded = load_model('cnn_model.h5')\n",
    "# evaluate model\n",
    "cnn_model_loaded.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n",
    "#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)\n",
    "    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[category],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "title (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "message_vec/embedding (Embed (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 52474)             6769146   \n",
      "=================================================================\n",
      "Total params: 21,988,794\n",
      "Trainable params: 6,988,794\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "53665/53665 [==============================] - 1050s 20ms/step - loss: 10.8894 - acc: 3.9132e-04\n",
      "Epoch 2/10\n",
      "53665/53665 [==============================] - 1098s 20ms/step - loss: 10.8660 - acc: 4.0995e-04\n",
      "Epoch 3/10\n",
      "53665/53665 [==============================] - 1105s 21ms/step - loss: 10.8420 - acc: 3.5405e-04\n",
      "Epoch 4/10\n",
      "53665/53665 [==============================] - 1110s 21ms/step - loss: 10.8139 - acc: 7.6400e-04\n",
      "Epoch 5/10\n",
      "53665/53665 [==============================] - 1112s 21ms/step - loss: 10.7881 - acc: 7.6400e-04\n",
      "Epoch 6/10\n",
      "53665/53665 [==============================] - 1114s 21ms/step - loss: 10.7727 - acc: 7.6400e-04\n",
      "Epoch 7/10\n",
      "53665/53665 [==============================] - 1111s 21ms/step - loss: 10.7596 - acc: 8.3854e-04\n",
      "Epoch 8/10\n",
      "53665/53665 [==============================] - 1085s 20ms/step - loss: 10.7536 - acc: 5.7766e-04\n",
      "Epoch 9/10\n",
      "53665/53665 [==============================] - 1047s 20ms/step - loss: 10.7463 - acc: 8.1990e-04\n",
      "Epoch 10/10\n",
      "53665/53665 [==============================] - 1090s 20ms/step - loss: 10.7374 - acc: 7.6400e-04\n",
      "5963/5963 [==============================] - 86s 14ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.46580704646324, 0.0008385041086701325]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)\n",
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model and architecture to single file\n",
    "lstm_model.save(\"lstm_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'lstm': lstm_model.predict(test_tokens[:100]),\n",
    "    'char_cnn': char_cnn_model.predict(test_char_vectors[:100]),\n",
    "    'cnn': cnn_model.predict(test_tokens[:100]),\n",
    "    'unigram': unigram_model.predict(test_tokens[:100]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "      <th>title</th>\n",
       "      <th>lstm</th>\n",
       "      <th>cnn</th>\n",
       "      <th>char_cnn</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beer type shrimp olive oil soy sauce fresh lime juice tabasco sauce tomato peeled seeded diced peeled grated gingerroot shop...</td>\n",
       "      <td>cold beer shrimp</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neelys bbq seasoning recipe follows crab boil seasoning recommended old bay tablespoons soy sauce egg lightly beaten cups ne...</td>\n",
       "      <td>bbq turkey meatloaf</td>\n",
       "      <td>caesar salad</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>gravlax with dill mayonnaise</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pint vanilla ice cream whole milk tablespoons bourbon teaspoons cane syrup molasses honey crumbled shortbread cookies tables...</td>\n",
       "      <td>bourbon pecan pie milk shake</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swiss chard stems removed leaves thinly sliced tablespoons extravirgin olive oil cups crusty bread cubeskosher salt freshly ...</td>\n",
       "      <td>seared steak with chard salad</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cups smoked turkey ham ground cups colby jack mozzarella grated onion finely choppedsalt freshly ground black pepper salt cl...</td>\n",
       "      <td>junes chile rellenos</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assorted mini squash patty pans andor baby zucchini tablespoons extravirgin olive oil frozen pearl onions pint cherry tomato...</td>\n",
       "      <td>sauteed mini vegetable medley</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>butter anchovy fillets drained chopped tablespoons chopped fresh italian parsley chopped fresh thyme leaves minced garlic mi...</td>\n",
       "      <td>cheese crostini with anchovy herb butter</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shrimp peeled deveined tablespoons canola oil kosher salt teaspoons freshly ground black pepper</td>\n",
       "      <td>perfectly grilled shrimp</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>outdoor grilled striped bass with vegetable tian and basil cream</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thin egg noodles angel hair spaghettini cups grated carrots cups bean sprouts rinsed drained cups minced scallion greens clo...</td>\n",
       "      <td>dan dan sesame noodles</td>\n",
       "      <td>chicken cacciatore</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>the ultimate breakfast for dinner sausage and spinach egg strata</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>soy sauce enough cover ribs head garlic minced sugar racks spare ribs</td>\n",
       "      <td>al frankens spare ribs</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>thin fries</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       ingredients  \\\n",
       "0  beer type shrimp olive oil soy sauce fresh lime juice tabasco sauce tomato peeled seeded diced peeled grated gingerroot shop...   \n",
       "1  neelys bbq seasoning recipe follows crab boil seasoning recommended old bay tablespoons soy sauce egg lightly beaten cups ne...   \n",
       "2  pint vanilla ice cream whole milk tablespoons bourbon teaspoons cane syrup molasses honey crumbled shortbread cookies tables...   \n",
       "3  swiss chard stems removed leaves thinly sliced tablespoons extravirgin olive oil cups crusty bread cubeskosher salt freshly ...   \n",
       "4  cups smoked turkey ham ground cups colby jack mozzarella grated onion finely choppedsalt freshly ground black pepper salt cl...   \n",
       "5  assorted mini squash patty pans andor baby zucchini tablespoons extravirgin olive oil frozen pearl onions pint cherry tomato...   \n",
       "6  butter anchovy fillets drained chopped tablespoons chopped fresh italian parsley chopped fresh thyme leaves minced garlic mi...   \n",
       "7                                  shrimp peeled deveined tablespoons canola oil kosher salt teaspoons freshly ground black pepper   \n",
       "8  thin egg noodles angel hair spaghettini cups grated carrots cups bean sprouts rinsed drained cups minced scallion greens clo...   \n",
       "9                                                            soy sauce enough cover ribs head garlic minced sugar racks spare ribs   \n",
       "\n",
       "                                      title                lstm  \\\n",
       "0                          cold beer shrimp           guacamole   \n",
       "1                       bbq turkey meatloaf        caesar salad   \n",
       "2              bourbon pecan pie milk shake           guacamole   \n",
       "3             seared steak with chard salad           guacamole   \n",
       "4                      junes chile rellenos           guacamole   \n",
       "5             sauteed mini vegetable medley           guacamole   \n",
       "6  cheese crostini with anchovy herb butter           guacamole   \n",
       "7                  perfectly grilled shrimp           guacamole   \n",
       "8                    dan dan sesame noodles  chicken cacciatore   \n",
       "9                    al frankens spare ribs           guacamole   \n",
       "\n",
       "              cnn  \\\n",
       "0    minute chili   \n",
       "1    minute chili   \n",
       "2    minute chili   \n",
       "3    minute chili   \n",
       "4    minute chili   \n",
       "5    minute chili   \n",
       "6    minute chili   \n",
       "7    minute chili   \n",
       "8    minute chili   \n",
       "9    minute chili   \n",
       "\n",
       "                                                           char_cnn  \\\n",
       "0                        spiced butternut squash stew with couscous   \n",
       "1                                      gravlax with dill mayonnaise   \n",
       "2                        spiced butternut squash stew with couscous   \n",
       "3                        spiced butternut squash stew with couscous   \n",
       "4                        spiced butternut squash stew with couscous   \n",
       "5                        spiced butternut squash stew with couscous   \n",
       "6                        spiced butternut squash stew with couscous   \n",
       "7  outdoor grilled striped bass with vegetable tian and basil cream   \n",
       "8  the ultimate breakfast for dinner sausage and spinach egg strata   \n",
       "9                                                        thin fries   \n",
       "\n",
       "          unigram  \n",
       "0    minute chili  \n",
       "1    minute chili  \n",
       "2    minute chili  \n",
       "3    minute chili  \n",
       "4    minute chili  \n",
       "5    minute chili  \n",
       "6    minute chili  \n",
       "7    minute chili  \n",
       "8    minute chili  \n",
       "9    minute chili  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe just for test data\n",
    "\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = df[training_count:training_count+100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'ingredients': test_df['ingredients'],\n",
    "    'title': test_df['title'],\n",
    "    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],\n",
    "    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],\n",
    "    'char_cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['char_cnn']],\n",
    "    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']]\n",
    "})\n",
    "eval_df = eval_df[['ingredients', 'title', 'lstm', 'cnn', 'char_cnn', 'unigram']]\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredients</th>\n",
       "      <th>title</th>\n",
       "      <th>lstm</th>\n",
       "      <th>cnn</th>\n",
       "      <th>char_cnn</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>beer type shrimp olive oil soy sauce fresh lime juice tabasco sauce tomato peeled seeded diced peeled grated gingerroot shop...</td>\n",
       "      <td>cold beer shrimp</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neelys bbq seasoning recipe follows crab boil seasoning recommended old bay tablespoons soy sauce egg lightly beaten cups ne...</td>\n",
       "      <td>bbq turkey meatloaf</td>\n",
       "      <td>caesar salad</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>gravlax with dill mayonnaise</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pint vanilla ice cream whole milk tablespoons bourbon teaspoons cane syrup molasses honey crumbled shortbread cookies tables...</td>\n",
       "      <td>bourbon pecan pie milk shake</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swiss chard stems removed leaves thinly sliced tablespoons extravirgin olive oil cups crusty bread cubeskosher salt freshly ...</td>\n",
       "      <td>seared steak with chard salad</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cups smoked turkey ham ground cups colby jack mozzarella grated onion finely choppedsalt freshly ground black pepper salt cl...</td>\n",
       "      <td>junes chile rellenos</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assorted mini squash patty pans andor baby zucchini tablespoons extravirgin olive oil frozen pearl onions pint cherry tomato...</td>\n",
       "      <td>sauteed mini vegetable medley</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>butter anchovy fillets drained chopped tablespoons chopped fresh italian parsley chopped fresh thyme leaves minced garlic mi...</td>\n",
       "      <td>cheese crostini with anchovy herb butter</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>spiced butternut squash stew with couscous</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shrimp peeled deveined tablespoons canola oil kosher salt teaspoons freshly ground black pepper</td>\n",
       "      <td>perfectly grilled shrimp</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>outdoor grilled striped bass with vegetable tian and basil cream</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thin egg noodles angel hair spaghettini cups grated carrots cups bean sprouts rinsed drained cups minced scallion greens clo...</td>\n",
       "      <td>dan dan sesame noodles</td>\n",
       "      <td>chicken cacciatore</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>the ultimate breakfast for dinner sausage and spinach egg strata</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>soy sauce enough cover ribs head garlic minced sugar racks spare ribs</td>\n",
       "      <td>al frankens spare ribs</td>\n",
       "      <td>guacamole</td>\n",
       "      <td>minute chili</td>\n",
       "      <td>thin fries</td>\n",
       "      <td>minute chili</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       ingredients  \\\n",
       "0  beer type shrimp olive oil soy sauce fresh lime juice tabasco sauce tomato peeled seeded diced peeled grated gingerroot shop...   \n",
       "1  neelys bbq seasoning recipe follows crab boil seasoning recommended old bay tablespoons soy sauce egg lightly beaten cups ne...   \n",
       "2  pint vanilla ice cream whole milk tablespoons bourbon teaspoons cane syrup molasses honey crumbled shortbread cookies tables...   \n",
       "3  swiss chard stems removed leaves thinly sliced tablespoons extravirgin olive oil cups crusty bread cubeskosher salt freshly ...   \n",
       "4  cups smoked turkey ham ground cups colby jack mozzarella grated onion finely choppedsalt freshly ground black pepper salt cl...   \n",
       "5  assorted mini squash patty pans andor baby zucchini tablespoons extravirgin olive oil frozen pearl onions pint cherry tomato...   \n",
       "6  butter anchovy fillets drained chopped tablespoons chopped fresh italian parsley chopped fresh thyme leaves minced garlic mi...   \n",
       "7                                  shrimp peeled deveined tablespoons canola oil kosher salt teaspoons freshly ground black pepper   \n",
       "8  thin egg noodles angel hair spaghettini cups grated carrots cups bean sprouts rinsed drained cups minced scallion greens clo...   \n",
       "9                                                            soy sauce enough cover ribs head garlic minced sugar racks spare ribs   \n",
       "\n",
       "                                      title                lstm  \\\n",
       "0                          cold beer shrimp           guacamole   \n",
       "1                       bbq turkey meatloaf        caesar salad   \n",
       "2              bourbon pecan pie milk shake           guacamole   \n",
       "3             seared steak with chard salad           guacamole   \n",
       "4                      junes chile rellenos           guacamole   \n",
       "5             sauteed mini vegetable medley           guacamole   \n",
       "6  cheese crostini with anchovy herb butter           guacamole   \n",
       "7                  perfectly grilled shrimp           guacamole   \n",
       "8                    dan dan sesame noodles  chicken cacciatore   \n",
       "9                    al frankens spare ribs           guacamole   \n",
       "\n",
       "              cnn  \\\n",
       "0    minute chili   \n",
       "1    minute chili   \n",
       "2    minute chili   \n",
       "3    minute chili   \n",
       "4    minute chili   \n",
       "5    minute chili   \n",
       "6    minute chili   \n",
       "7    minute chili   \n",
       "8    minute chili   \n",
       "9    minute chili   \n",
       "\n",
       "                                                           char_cnn  \\\n",
       "0                        spiced butternut squash stew with couscous   \n",
       "1                                      gravlax with dill mayonnaise   \n",
       "2                        spiced butternut squash stew with couscous   \n",
       "3                        spiced butternut squash stew with couscous   \n",
       "4                        spiced butternut squash stew with couscous   \n",
       "5                        spiced butternut squash stew with couscous   \n",
       "6                        spiced butternut squash stew with couscous   \n",
       "7  outdoor grilled striped bass with vegetable tian and basil cream   \n",
       "8  the ultimate breakfast for dinner sausage and spinach egg strata   \n",
       "9                                                        thin fries   \n",
       "\n",
       "          unigram  \n",
       "0    minute chili  \n",
       "1    minute chili  \n",
       "2    minute chili  \n",
       "3    minute chili  \n",
       "4    minute chili  \n",
       "5    minute chili  \n",
       "6    minute chili  \n",
       "7    minute chili  \n",
       "8    minute chili  \n",
       "9    minute chili  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "dictionary = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "allWordIndices = []\n",
    "for text in X_train:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train.factorize()[0], num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test.factorize()[0], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 29,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 100\n",
    "epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 33,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Training Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
