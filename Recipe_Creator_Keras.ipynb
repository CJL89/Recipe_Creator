{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing dependecies:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "from pandas import compat\n",
    "compat.PY3 = True\n",
    "\n",
    "# Configuring Notebook environment:\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.5)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>ingredients_vector</th>\n",
       "      <th>instructions_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p3pKOD6jIHEcjf20CCXohP8uqkG5dGi</th>\n",
       "      <td>grammie hamblets deviled crab</td>\n",
       "      <td>celery finely chopped green pepper finely chop...</td>\n",
       "      <td>toss ingredients lightly spoon buttered baking...</td>\n",
       "      <td>['celery', 'finely', 'chopped', 'green', 'pepp...</td>\n",
       "      <td>['toss', 'ingredients', 'lightly', 'spoon', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO</th>\n",
       "      <td>infineon raceway baked beans</td>\n",
       "      <td>skirt steak cut inch dicekosher salt fresh cra...</td>\n",
       "      <td>watch make recipe sprinkle steak salt pepper s...</td>\n",
       "      <td>['skirt', 'steak', 'cut', 'inch', 'dicekosher'...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'sprinkle', 'steak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK</th>\n",
       "      <td>southwestern black bean dip</td>\n",
       "      <td>cups dried black beans picked rinsed cups wate...</td>\n",
       "      <td>saucepan let beans soak enough cold water cove...</td>\n",
       "      <td>['cups', 'dried', 'black', 'beans', 'picked', ...</td>\n",
       "      <td>['saucepan', 'let', 'beans', 'soak', 'enough',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5l1yTSYFifF/M2dfbD6DX28WWQpLWNK</th>\n",
       "      <td>sour cream noodle bake</td>\n",
       "      <td>ground chuckone tomato sauce saltfreshly groun...</td>\n",
       "      <td>watch make recipe preheat oven degrees f brown...</td>\n",
       "      <td>['ground', 'chuckone', 'tomato', 'sauce', 'sal...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'preheat', 'oven',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kRBQSWtqYWqtkb34FGeenBSbC32gIdO</th>\n",
       "      <td>sushi renovation</td>\n",
       "      <td>rice brown mediumgrain cookedcup quinoacup swe...</td>\n",
       "      <td>special equipment sushi mat cook brown rice qu...</td>\n",
       "      <td>['rice', 'brown', 'mediumgrain', 'cookedcup', ...</td>\n",
       "      <td>['special', 'equipment', 'sushi', 'mat', 'cook...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         title  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  grammie hamblets deviled crab   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   infineon raceway baked beans   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    southwestern black bean dip   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         sour cream noodle bake   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               sushi renovation   \n",
       "\n",
       "                                                                       ingredients  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  celery finely chopped green pepper finely chop...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  skirt steak cut inch dicekosher salt fresh cra...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  cups dried black beans picked rinsed cups wate...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ground chuckone tomato sauce saltfreshly groun...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  rice brown mediumgrain cookedcup quinoacup swe...   \n",
       "\n",
       "                                                                      instructions  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  toss ingredients lightly spoon buttered baking...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  watch make recipe sprinkle steak salt pepper s...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  saucepan let beans soak enough cold water cove...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  watch make recipe preheat oven degrees f brown...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  special equipment sushi mat cook brown rice qu...   \n",
       "\n",
       "                                                                ingredients_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['celery', 'finely', 'chopped', 'green', 'pepp...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['cups', 'dried', 'black', 'beans', 'picked', ...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['ground', 'chuckone', 'tomato', 'sauce', 'sal...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['rice', 'brown', 'mediumgrain', 'cookedcup', ...   \n",
       "\n",
       "                                                               instructions_vector  \n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['toss', 'ingredients', 'lightly', 'spoon', 'b...  \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['watch', 'make', 'recipe', 'sprinkle', 'steak...  \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['saucepan', 'let', 'beans', 'soak', 'enough',...  \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['watch', 'make', 'recipe', 'preheat', 'oven',...  \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['special', 'equipment', 'sushi', 'mat', 'cook...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/strings/df_clean.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>ingredients_vector</th>\n",
       "      <th>instructions_vector</th>\n",
       "      <th>title_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p3pKOD6jIHEcjf20CCXohP8uqkG5dGi</th>\n",
       "      <td>grammie hamblets deviled crab</td>\n",
       "      <td>celery finely chopped green pepper finely chop...</td>\n",
       "      <td>toss ingredients lightly spoon buttered baking...</td>\n",
       "      <td>['celery', 'finely', 'chopped', 'green', 'pepp...</td>\n",
       "      <td>['toss', 'ingredients', 'lightly', 'spoon', 'b...</td>\n",
       "      <td>['grammie hamblets deviled crab']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO</th>\n",
       "      <td>infineon raceway baked beans</td>\n",
       "      <td>skirt steak cut inch dicekosher salt fresh cra...</td>\n",
       "      <td>watch make recipe sprinkle steak salt pepper s...</td>\n",
       "      <td>['skirt', 'steak', 'cut', 'inch', 'dicekosher'...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'sprinkle', 'steak...</td>\n",
       "      <td>['infineon raceway baked beans']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK</th>\n",
       "      <td>southwestern black bean dip</td>\n",
       "      <td>cups dried black beans picked rinsed cups wate...</td>\n",
       "      <td>saucepan let beans soak enough cold water cove...</td>\n",
       "      <td>['cups', 'dried', 'black', 'beans', 'picked', ...</td>\n",
       "      <td>['saucepan', 'let', 'beans', 'soak', 'enough',...</td>\n",
       "      <td>['southwestern black bean dip']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5l1yTSYFifF/M2dfbD6DX28WWQpLWNK</th>\n",
       "      <td>sour cream noodle bake</td>\n",
       "      <td>ground chuckone tomato sauce saltfreshly groun...</td>\n",
       "      <td>watch make recipe preheat oven degrees f brown...</td>\n",
       "      <td>['ground', 'chuckone', 'tomato', 'sauce', 'sal...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'preheat', 'oven',...</td>\n",
       "      <td>['sour cream noodle bake']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kRBQSWtqYWqtkb34FGeenBSbC32gIdO</th>\n",
       "      <td>sushi renovation</td>\n",
       "      <td>rice brown mediumgrain cookedcup quinoacup swe...</td>\n",
       "      <td>special equipment sushi mat cook brown rice qu...</td>\n",
       "      <td>['rice', 'brown', 'mediumgrain', 'cookedcup', ...</td>\n",
       "      <td>['special', 'equipment', 'sushi', 'mat', 'cook...</td>\n",
       "      <td>['sushi renovation']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         title  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  grammie hamblets deviled crab   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   infineon raceway baked beans   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    southwestern black bean dip   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         sour cream noodle bake   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               sushi renovation   \n",
       "\n",
       "                                                                       ingredients  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  celery finely chopped green pepper finely chop...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  skirt steak cut inch dicekosher salt fresh cra...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  cups dried black beans picked rinsed cups wate...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ground chuckone tomato sauce saltfreshly groun...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  rice brown mediumgrain cookedcup quinoacup swe...   \n",
       "\n",
       "                                                                      instructions  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  toss ingredients lightly spoon buttered baking...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  watch make recipe sprinkle steak salt pepper s...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  saucepan let beans soak enough cold water cove...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  watch make recipe preheat oven degrees f brown...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  special equipment sushi mat cook brown rice qu...   \n",
       "\n",
       "                                                                ingredients_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['celery', 'finely', 'chopped', 'green', 'pepp...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['cups', 'dried', 'black', 'beans', 'picked', ...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['ground', 'chuckone', 'tomato', 'sauce', 'sal...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['rice', 'brown', 'mediumgrain', 'cookedcup', ...   \n",
       "\n",
       "                                                               instructions_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['toss', 'ingredients', 'lightly', 'spoon', 'b...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['watch', 'make', 'recipe', 'sprinkle', 'steak...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['saucepan', 'let', 'beans', 'soak', 'enough',...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['watch', 'make', 'recipe', 'preheat', 'oven',...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['special', 'equipment', 'sushi', 'mat', 'cook...   \n",
       "\n",
       "                                                   title_tokenized  \n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['grammie hamblets deviled crab']  \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   ['infineon raceway baked beans']  \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    ['southwestern black bean dip']  \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         ['sour cream noodle bake']  \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               ['sushi renovation']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_tokenized'] = list(df['title'].apply(sent_tokenize).astype(str))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Splitting Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi    ['celery', 'finely', 'chopped', 'green', 'pepp...\n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO    ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...\n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    ['cups', 'dried', 'black', 'beans', 'picked', ...\n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK    ['ground', 'chuckone', 'tomato', 'sauce', 'sal...\n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO    ['rice', 'brown', 'mediumgrain', 'cookedcup', ...\n",
       "                                                         ...                        \n",
       "4bfMWxlbKhx/McJq/89k0SBdw.VvAzW    ['ears', 'fresh', 'corn', 'heads', 'belgian', ...\n",
       "T8lWBA1fcVdjxhMSWuoAbGoy5Lj.A8m    ['plum', 'tomatoessalt', 'sugar', 'zucchini', ...\n",
       "f/coffo2TMs2J2gq5nTOUIqH2TRAkui    ['tablespoons', 'olive', 'oil', 'tablespoons',...\n",
       "q3aDJc4zoEF5QT4e7Mn.ieQwV.DyHwS    ['ounces', 'butter', 'ounces', 'bittersweet', ...\n",
       "7cXA77UpdDtIfBug2v6lEVIuV3Zcvhm    ['cans', 'restaurantstyle', 'condensed', 'crab...\n",
       "Name: ingredients_vector, Length: 59612, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['ingredients_vector']\n",
    "y = df['title_tokenized']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47689 train sequences\n",
      "11923 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(y_train) + 1\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 100\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer.fit_on_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = []\n",
    "for text in X_train:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train.factorize()[0], num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test.factorize()[0], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (47689, 47690)\n",
      "y_test shape: (11923, 47690)\n"
     ]
    }
   ],
   "source": [
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (47689, 1000)\n",
      "X_test shape: (47689, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train on 42920 samples, validate on 4769 samples\n",
      "Epoch 1/10\n",
      "42920/42920 [==============================] - 479s 11ms/step - loss: 10.8646 - acc: 0.0028 - val_loss: 10.8707 - val_acc: 0.0080\n",
      "Epoch 2/10\n",
      "42920/42920 [==============================] - 486s 11ms/step - loss: 8.9801 - acc: 0.0396 - val_loss: 11.0910 - val_acc: 0.0258\n",
      "Epoch 3/10\n",
      "42920/42920 [==============================] - 498s 12ms/step - loss: 6.2879 - acc: 0.1647 - val_loss: 11.4266 - val_acc: 0.0447\n",
      "Epoch 4/10\n",
      "42920/42920 [==============================] - 1538s 36ms/step - loss: 3.7392 - acc: 0.3891 - val_loss: 11.9129 - val_acc: 0.0608\n",
      "Epoch 5/10\n",
      "42920/42920 [==============================] - 609s 14ms/step - loss: 2.1071 - acc: 0.6065 - val_loss: 12.4235 - val_acc: 0.0688\n",
      "Epoch 6/10\n",
      "42920/42920 [==============================] - 481s 11ms/step - loss: 1.2278 - acc: 0.7543 - val_loss: 12.7791 - val_acc: 0.0702\n",
      "Epoch 7/10\n",
      "42920/42920 [==============================] - 504s 12ms/step - loss: 0.8045 - acc: 0.8344 - val_loss: 13.1346 - val_acc: 0.0730\n",
      "Epoch 8/10\n",
      "42920/42920 [==============================] - 881s 21ms/step - loss: 0.5887 - acc: 0.8804 - val_loss: 13.2870 - val_acc: 0.0730\n",
      "Epoch 9/10\n",
      "42920/42920 [==============================] - 469s 11ms/step - loss: 0.4700 - acc: 0.9062 - val_loss: 13.3806 - val_acc: 0.0732\n",
      "Epoch 10/10\n",
      "42920/42920 [==============================] - 500s 12ms/step - loss: 0.4075 - acc: 0.9214 - val_loss: 13.5028 - val_acc: 0.0730\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 47689 input samples and 11923 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-85ca436c60b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m score = model.evaluate(X_test, y_test,\n\u001b[0;32m---> 31\u001b[0;31m                        batch_size=batch_size, verbose=1)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Springboard/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Springboard/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Springboard/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 47689 input samples and 11923 target samples."
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(256, input_shape=(max_words,), activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(2, input_shape=(num_classes), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    shuffle=True)\n",
    "\n",
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = define_model(max_words, num_classes)\n",
    "# fit model\n",
    "model.fit([X_train, X_train, X_train], np.array(y_train), epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
