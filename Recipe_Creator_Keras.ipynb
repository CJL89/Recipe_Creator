{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependecies:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "from pandas import compat\n",
    "compat.PY3 = True\n",
    "\n",
    "# Configuring Notebook environment:\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.5)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "      <th>ingredients_vector</th>\n",
       "      <th>instructions_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>p3pKOD6jIHEcjf20CCXohP8uqkG5dGi</th>\n",
       "      <td>grammie hamblets deviled crab</td>\n",
       "      <td>celery finely chopped green pepper finely chop...</td>\n",
       "      <td>toss ingredients lightly spoon buttered baking...</td>\n",
       "      <td>['celery', 'finely', 'chopped', 'green', 'pepp...</td>\n",
       "      <td>['toss', 'ingredients', 'lightly', 'spoon', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO</th>\n",
       "      <td>infineon raceway baked beans</td>\n",
       "      <td>skirt steak cut inch dicekosher salt fresh cra...</td>\n",
       "      <td>watch make recipe sprinkle steak salt pepper s...</td>\n",
       "      <td>['skirt', 'steak', 'cut', 'inch', 'dicekosher'...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'sprinkle', 'steak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK</th>\n",
       "      <td>southwestern black bean dip</td>\n",
       "      <td>cups dried black beans picked rinsed cups wate...</td>\n",
       "      <td>saucepan let beans soak enough cold water cove...</td>\n",
       "      <td>['cups', 'dried', 'black', 'beans', 'picked', ...</td>\n",
       "      <td>['saucepan', 'let', 'beans', 'soak', 'enough',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5l1yTSYFifF/M2dfbD6DX28WWQpLWNK</th>\n",
       "      <td>sour cream noodle bake</td>\n",
       "      <td>ground chuckone tomato sauce saltfreshly groun...</td>\n",
       "      <td>watch make recipe preheat oven degrees f brown...</td>\n",
       "      <td>['ground', 'chuckone', 'tomato', 'sauce', 'sal...</td>\n",
       "      <td>['watch', 'make', 'recipe', 'preheat', 'oven',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kRBQSWtqYWqtkb34FGeenBSbC32gIdO</th>\n",
       "      <td>sushi renovation</td>\n",
       "      <td>rice brown mediumgrain cookedcup quinoacup swe...</td>\n",
       "      <td>special equipment sushi mat cook brown rice qu...</td>\n",
       "      <td>['rice', 'brown', 'mediumgrain', 'cookedcup', ...</td>\n",
       "      <td>['special', 'equipment', 'sushi', 'mat', 'cook...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         title  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  grammie hamblets deviled crab   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO   infineon raceway baked beans   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    southwestern black bean dip   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK         sour cream noodle bake   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO               sushi renovation   \n",
       "\n",
       "                                                                       ingredients  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  celery finely chopped green pepper finely chop...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  skirt steak cut inch dicekosher salt fresh cra...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  cups dried black beans picked rinsed cups wate...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ground chuckone tomato sauce saltfreshly groun...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  rice brown mediumgrain cookedcup quinoacup swe...   \n",
       "\n",
       "                                                                      instructions  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  toss ingredients lightly spoon buttered baking...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  watch make recipe sprinkle steak salt pepper s...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  saucepan let beans soak enough cold water cove...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  watch make recipe preheat oven degrees f brown...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  special equipment sushi mat cook brown rice qu...   \n",
       "\n",
       "                                                                ingredients_vector  \\\n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['celery', 'finely', 'chopped', 'green', 'pepp...   \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['skirt', 'steak', 'cut', 'inch', 'dicekosher'...   \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['cups', 'dried', 'black', 'beans', 'picked', ...   \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['ground', 'chuckone', 'tomato', 'sauce', 'sal...   \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['rice', 'brown', 'mediumgrain', 'cookedcup', ...   \n",
       "\n",
       "                                                               instructions_vector  \n",
       "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi  ['toss', 'ingredients', 'lightly', 'spoon', 'b...  \n",
       "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO  ['watch', 'make', 'recipe', 'sprinkle', 'steak...  \n",
       "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK  ['saucepan', 'let', 'beans', 'soak', 'enough',...  \n",
       "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK  ['watch', 'make', 'recipe', 'preheat', 'oven',...  \n",
       "kRBQSWtqYWqtkb34FGeenBSbC32gIdO  ['special', 'equipment', 'sushi', 'mat', 'cook...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/strings/df_clean.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ingredients = gs.Word2Vec(df['ingredients_vector'], min_count=1, size= 25, workers=3, window=3, sg=1)\n",
    "# model_instructions = gs.Word2Vec(df['instructions_vector'], min_count=1, size= 25, workers=3, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_ingredients)\n",
    "# data = model_instructions.most_similar('beef')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p3pKOD6jIHEcjf20CCXohP8uqkG5dGi    celery finely chopped green pepper finely chop...\n",
      "S7aeOIrsrgT0jLP32jKGg4j.o9zi2DO    skirt steak cut inch dicekosher salt fresh cra...\n",
      "o9MItV9txfoPsUQ4v8b0vh1.VdjwfsK    cups dried black beans picked rinsed cups wate...\n",
      "5l1yTSYFifF/M2dfbD6DX28WWQpLWNK    ground chuckone tomato sauce saltfreshly groun...\n",
      "kRBQSWtqYWqtkb34FGeenBSbC32gIdO    rice brown mediumgrain cookedcup quinoacup swe...\n",
      "Name: ingredients, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X = df['ingredients']\n",
    "y = df['title']\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 338\n",
      "Vocabulary size: 45441\n",
      "(59612, 338)\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(X)\n",
    "# calculate max document length\n",
    "length = max_length(X)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, y, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 338)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 338)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 338)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 338, 100)     4544100     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 338, 100)     4544100     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 338, 100)     4544100     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 335, 32)      12832       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 333, 32)      19232       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 331, 32)      25632       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 335, 32)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 333, 32)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 331, 32)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 167, 32)      0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 166, 32)      0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 165, 32)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 5344)         0           max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 5312)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 5280)         0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 15936)        0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 10)           159370      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            11          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,849,377\n",
      "Trainable params: 13,849,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], np.array(y), epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "tokenize = text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenize.texts_to_matrix(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelBinarizer()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "          optimizer='adam', \n",
    "          metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=100, \n",
    "                    epochs=2, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
